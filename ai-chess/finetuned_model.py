# -*- coding: utf-8 -*-
"""finetuned-model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aT07xkoE_JYKe7Wc7mbQHbiG6AJrjOcF
"""

# !pip install -q unsloth unsloth_zoo
# !pip install -q --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git

from google.colab import userdata
HF_TOKEN = userdata.get('HF_TOKEN')

import torch
print(torch.cuda.is_available())
print(torch.cuda.device_count())
print(torch.version.cuda)

from unsloth import FastLanguageModel
import torch
max_seq_length = 2048 # Choose any! Unsloth also supports RoPE (Rotary Positinal Embedding) scaling internally.
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/Llama-3.2-3B-Instruct",
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit, # Will load the 4Bit Quantized Model
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16, # a higher alpha value assigns more weight to the LoRA activations
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,
    loftq_config = None,
)

import pandas as pd
data_path = "/content/drive/MyDrive/Colab Notebooks/small_dataset.csv"
df = pd.read_csv(data_path)
df

r1_prompt = """You are a strong chess engine. Given a chess position in FEN notation, predict the single best legal move for the current player in UCI format. Output only the move.
<movetext>
{}
</movetext>

<white_elo>
{}
</white_elo>

<black_elo>
{}
</black_elo>

<white_diff>
{}
</white_diff>

<black_diff>
{}
</black_diff>

<opening>
{}
</opening>

<eco>
{}
</eco>
"""
EOS_TOKEN = tokenizer.eos_token

def formatting_prompts_func(examples):
  movetext = examples["movetext"]
  white_elo = examples["white_elo"]
  black_elo = examples["black_elo"]
  white_diff = examples['white_diff']
  black_diff = examples['black_diff']
  opening = examples['opening']
  eco = examples['eco']
  texts = []

  for movetext, white_elo, black_elo, white_diff,black_diff,opening,eco in zip(movetext, white_elo, black_elo , white_diff , black_diff , opening , eco):
    text = r1_prompt.format(movetext, white_elo, black_elo , white_diff , black_diff , opening , eco)+EOS_TOKEN
    texts.append(text)

  return {"text": texts}
from datasets import Dataset

hf_dataset = Dataset.from_pandas(df)
hf_dataset = hf_dataset.map(formatting_prompts_func, batched=True)

from trl import SFTTrainer
from transformers import TrainingArguments, DataCollatorForSeq2Seq
from unsloth import is_bfloat16_supported

trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = hf_dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2, # Number of processors to use for processing the dataset
    packing = False, # Can make training 5x faster for short sequences.
    args = TrainingArguments(
        per_device_train_batch_size = 2, # The batch size per GPU/TPU core
        gradient_accumulation_steps = 4, # Number of steps to perform befor each gradient accumulation
        warmup_steps = 5, # Few updates with low learning rate before actual training
        max_steps = 60, # Specifies the total number of training steps (batches) to run.
        learning_rate = 2e-4,
        fp16 = not is_bfloat16_supported(),
        bf16 = is_bfloat16_supported(),
        logging_steps = 1,
        optim = "adamw_8bit", # Optimizer
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
        report_to = "none", # Use this for WandB etc for observability
    ),
)

trainer_stats = trainer.train()

from unsloth.chat_templates import get_chat_template
sys_prompt = """You are a strong chess engine. Given a chess position in FEN notation, predict the single best legal move for the current player in UCI format. Output only the move.
<problem>
{}
</problem>
"""
message = sys_prompt.format("""
1. b3 c5 2. Bb2 d6 3. d3 f6 4. Nd2 e5 5. c4 Bd7 6. f3 Be7 7. h3 Nh6 8. Qc2 Nf5 9. O-O-O Bf8 10. Ne4 Ne7 11. Nxd6# 1-0""")
tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3.1",
)
FastLanguageModel.for_inference(model) # Enable native 2x faster inference

messages = [
    {"role": "user", "content": message},
]
inputs = tokenizer.apply_chat_template(
    messages,
    tokenize = True,
    add_generation_prompt = True, # Must add for generation
    return_tensors = "pt",
).to("cuda")

outputs = model.generate(input_ids = inputs, max_new_tokens = 1024, use_cache = True,
                         temperature = 1.5, min_p = 0.1)# inferencing
response = tokenizer.batch_decode(outputs)

print(response[0])

outputs

model.save_pretrained("/content/chess-ai-merged", safe_serialization=True)
tokenizer.save_pretrained("/content/chess-ai-merged")

# !ls /content/chess-ai-merged

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

base_model_name = "unsloth/Llama-3.2-3B-Instruct"  # example: "Qwen/Qwen2.5-3B"
adapter_path = "/content/chess-ai"

tokenizer = AutoTokenizer.from_pretrained(base_model_name)

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

model = PeftModel.from_pretrained(base_model, adapter_path)

# ðŸ”´ THIS IS THE MOST IMPORTANT LINE
model = model.merge_and_unload()

# âœ… NOW save the REAL model
model.save_pretrained("/content/chess-ai-merged", safe_serialization=True)
tokenizer.save_pretrained("/content/chess-ai-merged")

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/llama.cpp

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/llama.cpp/convert

# !python3 convert_hf_to_gguf.py \
#   --outtype f16 \
#   /content/chess-ai-merged \
#   --outfile /content/chess-ai.f16.gguf

# !ls /content | grep gguf

# # Commented out IPython magic to ensure Python compatibility.
# # %cd /content/llama.cpp/build/bin
# !ls

# !./llama-quantize \
#   /content/chess-ai.f16.gguf \
#   /content/model-q5_k.gguf \
#   q5_k

# !find /content/llama.cpp -type f -name "*quantize*"

# !llama-quantize chess-ai.f16.gguf model-q5_k.gguf q5_k
# !ls /content/llama.cpp/build/bin

model.save_pretrained_gguf("chess-ai-001-3B-GGUF", tokenizer)

# !ls build/bin

# !./quantize /content/chess-ai.gguf /content/model-q5_k.gguf q5_k

import subprocess
subprocess.Popen(["ollama", "serve"])
import time
time.sleep(3)

print(tokenizer._ollama_modelfile)

# !ollama create unsloth_model -f ./chess-ai-001-3B-GGUF/Modelfile
# !ls /content/chess-ai-001-3B-GGUF

from google.colab import files

files.download("/content/chess-ai-001-3B-GGUF/model-q5_k.gguf")